version: '3.0'

services:
  app:
    # image: 
    build:
      context: .
      # dockerfile: ./cuda.Dockerfile
      dockerfile: ./ubuntu.Dockerfile
    restart: always
    # runtime: nvidia
    ######################################  origin: gpu
    # deploy:
    #   resources:
    #     reservations:
    #         devices:
    #           - driver: nvidia
    #             # count: 1
    #             capabilities: [gpu]
    ######################################  origin: gpu
    ports:
      - "${SSH_PORT}:22"
    volumes:
      - ./langchain:/code
      - ./py:/py
      # ln -s /home/faith/miniconda3 /py
      - ./embedding_models:/code/embedding_models
      # ln -s /home/faith/.cache/torch/sentence_transformers/ embedding_models
    container_name: cuda11
    env_file:
      - ./.env
    entrypoint:
      ["/bin/bash", "-c", "/bin/bash /opt/start.sh"]

  